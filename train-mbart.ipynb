{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11788176,"sourceType":"datasetVersion","datasetId":6742498}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !python -m wikiextractor.WikiExtractor /kaggle/input/olo-wiki/olowiki-20250201-pages-articles-multistream.xml","metadata":{"execution":{"iopub.status.busy":"2025-02-27T12:34:21.674169Z","iopub.execute_input":"2025-02-27T12:34:21.674498Z","iopub.status.idle":"2025-02-27T12:34:21.678161Z","execution_failed":"2025-02-27T12:40:15.522Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os\n# import subprocess\n# from IPython.display import FileLink, display\n\n# def download_file(path, download_file_name):\n#     os.chdir('/kaggle/working/')\n#     zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n#     command = f\"zip {zip_name} {path} -r\"\n#     result = subprocess.run(command, shell=True, capture_output=True, text=True)\n#     if result.returncode != 0:\n#         print(\"Unable to run zip command!\")\n#         print(result.stderr)\n#         return\n#     display(FileLink(f'{download_file_name}.zip'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:28:00.665570Z","iopub.execute_input":"2025-03-06T08:28:00.665854Z","iopub.status.idle":"2025-03-06T08:28:00.670817Z","shell.execute_reply.started":"2025-03-06T08:28:00.665833Z","shell.execute_reply":"2025-03-06T08:28:00.669892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# download_file('/kaggle/working/nllb-rus-kar/model.safetensors', 'weights')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T08:28:03.400414Z","iopub.execute_input":"2025-03-06T08:28:03.400922Z","iopub.status.idle":"2025-03-06T08:30:15.458096Z","shell.execute_reply.started":"2025-03-06T08:28:03.400873Z","shell.execute_reply":"2025-03-06T08:30:15.457380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install sacremoses sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:10:33.221005Z","iopub.execute_input":"2025-03-11T23:10:33.221388Z","iopub.status.idle":"2025-03-11T23:10:38.488135Z","shell.execute_reply.started":"2025-03-11T23:10:33.221355Z","shell.execute_reply":"2025-03-11T23:10:38.487223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\nimport re\nimport random\nimport sys\nfrom collections import Counter\nimport unicodedata\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm, trange\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, NllbTokenizer\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom transformers import get_constant_schedule_with_warmup\nfrom transformers.optimization import Adafactor\n\nimport sentencepiece as spm\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n\nfrom torch.optim.lr_scheduler import SequentialLR, ExponentialLR, ConstantLR, LinearLR\nfrom sacremoses import MosesPunctNormalizer\n\nimport json\nimport os\nimport shutil\nfrom typing import List, Tuple\n\nfrom transformers.models.nllb.tokenization_nllb import FAIRSEQ_LANGUAGE_CODES\n\ndef cleanup():\n    \"\"\"Try to free GPU memory\"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:10:38.489534Z","iopub.execute_input":"2025-03-11T23:10:38.489801Z","iopub.status.idle":"2025-03-11T23:10:59.018118Z","shell.execute_reply.started":"2025-03-11T23:10:38.489778Z","shell.execute_reply":"2025-03-11T23:10:59.017155Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KAGGLE_INPUT = '/kaggle/input/'\nMODEL_PATH = 'mbart-rus-kar'\nDF_PATH = KAGGLE_INPUT + 'karelian-data/parallel_df_merged4.csv'\n# MODEL_LOAD_PATH = KAGGLE_INPUT + '/m/taciturno/nllb-rus-kar/pytorch/ft-messy/1'\nMODEL_SAVE_PATH = MODEL_PATH","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:11:02.566256Z","iopub.execute_input":"2025-03-11T23:11:02.566595Z","iopub.status.idle":"2025-03-11T23:11:02.570454Z","shell.execute_reply.started":"2025-03-11T23:11:02.566571Z","shell.execute_reply":"2025-03-11T23:11:02.569719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MBART_PATH = '/kaggle/input/mbart-rus-kar/pytorch/15k/1'\nNLLB_OLD_PATH = '/kaggle/input/nllb-rus-kar/pytorch/old-data/1'\nNLLB_NEW_PATH = '/kaggle/input/nllb-rus-kar/pytorch/new-data/3'\nNLLB_LAST_PATH = '/kaggle/working/nllb-rus-kar'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:11:03.082256Z","iopub.execute_input":"2025-03-11T23:11:03.082570Z","iopub.status.idle":"2025-03-11T23:11:03.086379Z","shell.execute_reply.started":"2025-03-11T23:11:03.082547Z","shell.execute_reply":"2025-03-11T23:11:03.085311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_corpus_labeled = pd.read_csv(DF_PATH)\ndf_train = df_corpus_labeled[df_corpus_labeled.split=='train'].copy() # 22692 items\ndf_dev = df_corpus_labeled[df_corpus_labeled.split=='dev'].copy()     # 500 items\ndf_test = df_corpus_labeled[df_corpus_labeled.split=='test'].copy()  \n\ndf_dev = df_dev.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:20:47.386899Z","iopub.execute_input":"2025-03-11T23:20:47.387233Z","iopub.status.idle":"2025-03-11T23:20:47.720224Z","shell.execute_reply.started":"2025-03-11T23:20:47.387211Z","shell.execute_reply":"2025-03-11T23:20:47.719221Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ONLY OLD DATA (content-krl-20)\n\n# train_path = KAGGLE_INPUT + \"content-krl-20/train/\"\n# dev_path = KAGGLE_INPUT + \"content-krl-20/test/\"\n# with open(train_path + 'train.krl.txt', \"r\", encoding=\"utf-8\") as file:\n#     kar_corpus = file.readlines()\n# kar_corpus = [l.replace('\\n', '').strip() for l in kar_corpus]\n# with open(train_path + 'train.rus.txt', \"r\", encoding=\"utf-8\") as file:\n#     rus_corpus = file.readlines()\n# rus_corpus = [l.replace('\\n', '').strip() for l in rus_corpus]\n# df_train = pd.DataFrame([kar_corpus, rus_corpus]).T\n# df_train.columns = ['kar', 'rus']\n\n# with open(dev_path + 'dev.krl.txt', \"r\", encoding=\"utf-8\") as file:\n#     kar_corpus = file.readlines()\n# kar_corpus = [l.replace('\\n', '').strip() for l in kar_corpus]\n# with open(dev_path + 'dev.rus.txt', \"r\", encoding=\"utf-8\") as file:\n#     rus_corpus = file.readlines()\n# rus_corpus = [l.replace('\\n', '').strip() for l in rus_corpus]\n# df_dev = pd.DataFrame([kar_corpus, rus_corpus]).T\n# df_dev.columns = ['kar', 'rus']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T14:30:38.440832Z","iopub.execute_input":"2025-03-03T14:30:38.441188Z","iopub.status.idle":"2025-03-03T14:30:38.763330Z","shell.execute_reply.started":"2025-03-03T14:30:38.441122Z","shell.execute_reply":"2025-03-03T14:30:38.762406Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preproc","metadata":{}},{"cell_type":"code","source":"mpn = MosesPunctNormalizer(lang=\"en\")\nmpn.substitutions = [\n    (re.compile(r), sub) for r, sub in mpn.substitutions\n]\n\ndef get_non_printing_char_replacer(replace_by: str = \" \"):\n    non_printable_map = {\n        ord(c): replace_by\n        for c in (chr(i) for i in range(sys.maxunicode + 1))\n        # same as \\p{C} in perl\n        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n    }\n\n    def replace_non_printing_char(line) -> str:\n        return line.translate(non_printable_map)\n\n    return replace_non_printing_char\n\nreplace_nonprint = get_non_printing_char_replacer(\" \")\n\ndef preproc(text):\n    clean = mpn.normalize(text)\n    clean = replace_nonprint(clean)\n    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n    clean = unicodedata.normalize(\"NFKC\", clean)\n    return clean","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:14:46.774065Z","iopub.execute_input":"2025-03-11T23:14:46.774472Z","iopub.status.idle":"2025-03-11T23:14:47.258672Z","shell.execute_reply.started":"2025-03-11T23:14:46.774431Z","shell.execute_reply":"2025-03-11T23:14:47.257534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tokenizer","metadata":{}},{"cell_type":"code","source":"# additional_special_tokens=sorted(FAIRSEQ_LANGUAGE_CODES + ['ol_KA'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:17:01.710538Z","iopub.execute_input":"2025-03-11T23:17:01.710874Z","iopub.status.idle":"2025-03-11T23:17:01.714708Z","shell.execute_reply.started":"2025-03-11T23:17:01.710850Z","shell.execute_reply":"2025-03-11T23:17:01.713783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\nmodel = MBartForConditionalGeneration.from_pretrained(MODEL_NAME)\ntokenizer = MBart50TokenizerFast.from_pretrained(MODEL_NAME)\nlen(tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:20:31.376132Z","iopub.execute_input":"2025-03-11T23:20:31.376695Z","iopub.status.idle":"2025-03-11T23:20:39.012218Z","shell.execute_reply.started":"2025-03-11T23:20:31.376661Z","shell.execute_reply":"2025-03-11T23:20:39.011339Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train","metadata":{}},{"cell_type":"code","source":"LR = 1e-4\n\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=LR,\n    clip_threshold=1.0,\n    weight_decay=1e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\nscheduler1 = ConstantLR(optimizer, factor=1, total_iters=1000)\nscheduler2 = ConstantLR(optimizer, factor=0.9, total_iters=8000)\nscheduler3 = ConstantLR(optimizer, factor=0.8, total_iters=1000)\nscheduler4 = ConstantLR(optimizer, factor=0.7, total_iters=4000)\nscheduler = SequentialLR(optimizer, \n                         schedulers=[scheduler1, scheduler2, scheduler3, scheduler4], \n                         milestones=[1000,9000,10000])\n# scheduler1 = ConstantLR(optimizer, factor=0.9, total_iters=100)\n# scheduler2 = ExponentialLR(optimizer, gamma=0.9)\n# scheduler = LinearLR(optimizer, start_factor=0.5, total_iters=1400)\n# scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[100])\n# scheduler = scheduler1","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:20:39.013436Z","iopub.execute_input":"2025-03-11T23:20:39.013753Z","iopub.status.idle":"2025-03-11T23:20:40.129265Z","shell.execute_reply.started":"2025-03-11T23:20:39.013732Z","shell.execute_reply":"2025-03-11T23:20:40.128547Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LANGS = [('rus', 'ru_RU'), ('kar', 'fi_FI')]\n# TODO переделать в dataset\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, lang1), (l2, lang2) = random.sample(LANGS, 2) # здесь random чтобы переводила модель туда-сюда\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(preproc(item[l1]))\n        yy.append(preproc(item[l2]))\n    return xx, yy, lang1, lang2\n\nprint(get_batch_pairs(1))","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:21:01.117891Z","iopub.execute_input":"2025-03-11T23:21:01.118233Z","iopub.status.idle":"2025-03-11T23:21:01.125019Z","shell.execute_reply.started":"2025-03-11T23:21:01.118209Z","shell.execute_reply":"2025-03-11T23:21:01.124010Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 16  # 32 already doesn't fit well to 15GB of GPU memory\nMAX_LENGTH = 128 \nTRAINING_STEPS = 14000 \n\nN_STEPS_TO_ESTIMATE = 1000\nlosses = list()\n\npreproc(df_dev.iloc[0]['kar'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:21:05.171036Z","iopub.execute_input":"2025-03-11T23:21:05.171380Z","iopub.status.idle":"2025-03-11T23:21:05.177488Z","shell.execute_reply.started":"2025-03-11T23:21:05.171349Z","shell.execute_reply":"2025-03-11T23:21:05.176617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_batched_validation(batch_size):\n    (l1, lang1), (l2, lang2) = random.sample(LANGS, 2)\n    for i in range(0, len(df_dev), batch_size):\n        xx, yy = list(), list()\n        sl = df_dev.iloc[i:i+batch_size]\n        for _, row in sl.iterrows():\n            xx.append(preproc(row[l1]))\n            yy.append(preproc(row[l2]))\n        yield xx, yy, lang1, lang2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:00:24.032865Z","iopub.execute_input":"2025-03-11T23:00:24.033175Z","iopub.status.idle":"2025-03-11T23:00:24.038609Z","shell.execute_reply.started":"2025-03-11T23:00:24.033150Z","shell.execute_reply":"2025-03-11T23:00:24.037650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validation(model, data=df_dev, batch_size=16):\n    model.eval()\n    av_loss = list()\n    with torch.no_grad():\n        for xx, yy, lang1, lang2 in get_batched_validation(batch_size):#range(len(data) // batch_size):\n            # xx, yy, lang1, lang2 =  #get_batch_pairs(batch_size, data)\n            tokenizer.src_lang = lang1\n            x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to('cuda')\n            tokenizer.src_lang = lang2\n            y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to('cuda')\n            y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100 # TODO: надо ли?\n            loss = model(**x, labels=y.input_ids).loss\n            av_loss.append(loss.item())\n    model.train()\n    return np.mean(av_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T23:00:29.504655Z","iopub.execute_input":"2025-03-11T23:00:29.504990Z","iopub.status.idle":"2025-03-11T23:00:29.510795Z","shell.execute_reply.started":"2025-03-11T23:00:29.504961Z","shell.execute_reply":"2025-03-11T23:00:29.510017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T22:23:41.648559Z","iopub.execute_input":"2025-03-05T22:23:41.648828Z","iopub.status.idle":"2025-03-05T22:23:41.661529Z","shell.execute_reply.started":"2025-03-05T22:23:41.648801Z","shell.execute_reply":"2025-03-05T22:23:41.660892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# get_lr(optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:02:02.950438Z","iopub.execute_input":"2025-03-05T16:02:02.950790Z","iopub.status.idle":"2025-03-05T16:02:02.954571Z","shell.execute_reply.started":"2025-03-05T16:02:02.950760Z","shell.execute_reply":"2025-03-05T16:02:02.953577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\nx, y, loss = None, None, None\nBEST_VAL_LOSS = 1e9\ntrain_loss = 1e9\ncleanup()\n\n\ntq = trange(len(losses), TRAINING_STEPS)\nfor step in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(BATCH_SIZE)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e: \n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n    # if step % 100 == 0:\n    #     print(get_lr(optimizer))\n    if step < 9000 and step % N_STEPS_TO_ESTIMATE == 0:\n        # average for N steps\n        train_loss = np.mean(losses[-N_STEPS_TO_ESTIMATE:])\n        val_loss = validation(model)\n        print(f'Train loss is {train_loss} and valid loss is {val_loss} at step {step}')\n        if train_loss < 1.5 and val_loss < BEST_VAL_LOSS:\n            BEST_VAL_LOSS = val_loss\n            model.save_pretrained(MODEL_SAVE_PATH)\n            tokenizer.save_pretrained(MODEL_SAVE_PATH)\n    elif step >= 9000 and step % 100 == 0:\n        train_loss = np.mean(losses[-100:])\n        val_loss = validation(model)\n        print(f'Train loss is {train_loss} and valid loss is {val_loss} at step {step}')\n        if train_loss < 1.5 and val_loss < BEST_VAL_LOSS:\n            BEST_VAL_LOSS = val_loss\n            model.save_pretrained(MODEL_SAVE_PATH)\n            tokenizer.save_pretrained(MODEL_SAVE_PATH)","metadata":{"execution":{"iopub.status.busy":"2025-03-11T23:03:40.430733Z","iopub.execute_input":"2025-03-11T23:03:40.431089Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
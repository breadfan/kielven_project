{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b772dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, NllbTokenizer\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "from tqdm.auto import trange\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ee82e",
   "metadata": {},
   "source": [
    "# Tartu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd83ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tartu_trans(text):\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "    json_data = {\n",
    "        'text': text,\n",
    "        'src': 'olo',\n",
    "        'tgt': 'rus',\n",
    "        'domain': 'general',\n",
    "        'application': 'Documentation UI',\n",
    "    }\n",
    "    response = requests.post('https://api.tartunlp.ai/translation/v2', headers=headers, json=json_data)\n",
    "    return response.json()['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc76960e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Средняя высота поверхности Антарктиды - самая высокая среди всех континентов.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tartu_trans('Antarktidan pinnan keskikorgevus on kaikkien manderien suurin.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a9d443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('data', 'corpus_to_label.txt')\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4bfde9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 12000\n",
    "tartu_translates = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f1b9e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12000 [00:42<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for line_id in tqdm(range(N)):\n",
    "    text = lines[line_id].strip()\n",
    "    if len(text.split()) != 1:\n",
    "        try:\n",
    "            trans = get_tartu_trans(text)\n",
    "            tartu_translates.append(trans)\n",
    "        except:\n",
    "            print(f'Error on {line_id}')\n",
    "            break\n",
    "    if len(tartu_translates) >= 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478a43f",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d9ee576",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','saved_dictionary.pkl'), 'rb') as f:\n",
    "    kar_to_rus = pickle.load(f)\n",
    "dict_embeds = np.load('data/dict_embeds.npy')\n",
    "dict_keys = list(kar_to_rus.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025da7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NLLB_NEW_PATH = 'weights/nllb/'\n",
    "nllb_model = AutoModelForSeq2SeqLM.from_pretrained(NLLB_NEW_PATH)\n",
    "nllb_tokenizer = NllbTokenizer.from_pretrained(NLLB_NEW_PATH)\n",
    "\n",
    "MBART_PATH = 'weights/mbart/'\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained(MBART_PATH)\n",
    "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(MBART_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f466a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embed(text, model, tokenizer, src_lang, tgt_lang):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        res = model.generate(**t, \n",
    "                              return_dict_in_generate=True, \n",
    "                              forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "                              output_hidden_states=True)\n",
    "        per_token_embeddings = res['encoder_hidden_states'][-1]\n",
    "        mask = t.attention_mask\n",
    "        embeddings = (per_token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.unsqueeze(-1).sum(1)\n",
    "        # normalize\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings.squeeze(-1).cpu().numpy()\n",
    "\n",
    "def batched_embed(texts, batch_size=16, **kwargs):\n",
    "    \"\"\"Translate texts in batches of similar length\"\"\"\n",
    "    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n",
    "    results = []\n",
    "    for i in trange(0, len(texts2), batch_size):\n",
    "        results.extend(np.array(get_embed(texts2[i: i+batch_size], **kwargs)))\n",
    "    return np.array([p for i, p in sorted(zip(idxs, results))])\n",
    "\n",
    "def translate(\n",
    "    text, \n",
    "    model, tokenizer,\n",
    "    src_lang='rus_Cyrl', tgt_lang='eng_Latn', \n",
    "    a=32, b=3, max_input_length=1024, num_beams=4, \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors='pt', padding=True, truncation=True, \n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    model.eval() # turn off training mode\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        num_beams=num_beams, **kwargs\n",
    "    )\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)\n",
    "\n",
    "def batched_translate(texts, batch_size=16, **kwargs):\n",
    "    \"\"\"Translate texts in batches of similar length\"\"\"\n",
    "    idxs, texts2 = zip(*sorted(enumerate(texts), key=lambda p: len(p[1]), reverse=True))\n",
    "    results = []\n",
    "    for i in trange(0, len(texts2), batch_size):\n",
    "        results.extend(translate(texts2[i: i+batch_size], **kwargs))\n",
    "    return [p for i, p in sorted(zip(idxs, results))]\n",
    "\n",
    "def get_dict_translate(text):\n",
    "    text_split = text.split() \n",
    "    if len(text_split) > 1:\n",
    "        embeds = batched_embed(text_split, \n",
    "                                    model=nllb_model, \n",
    "                                    tokenizer=nllb_tokenizer, \n",
    "                                    src_lang='olo_Latn', \n",
    "                                    tgt_lang='rus_Cyrl')\n",
    "    else:\n",
    "        embeds = get_embed(text_split,\n",
    "                                model=nllb_model, \n",
    "                                tokenizer=nllb_tokenizer, \n",
    "                                src_lang='olo_Latn', \n",
    "                                tgt_lang='rus_Cyrl')\n",
    "    if embeds.ndim == 1: # если одномерный массив\n",
    "        embeds = embeds.reshape(1,-1)\n",
    "    ids_closest = cosine_similarity(embeds, dict_embeds).argmax(axis=1)\n",
    "    kar_words = [kar_to_rus[dict_keys[id_closest]] for id_closest in ids_closest]\n",
    "    pairs = list(zip(text_split, kar_words))\n",
    "    return pairs\n",
    "\n",
    "def get_translates(texts):\n",
    "    if isinstance(texts, str):\n",
    "        nllb_trans = translate(texts, nllb_model, nllb_tokenizer, src_lang='olo_Latn', tgt_lang='rus_Cyrl')\n",
    "        mbart_trans = translate(texts, mbart_model, mbart_tokenizer, src_lang='fi_FI', tgt_lang='ru_RU')\n",
    "        pairs = get_dict_translate(texts)\n",
    "    else:\n",
    "        nllb_trans = batched_translate(texts, 16, model=nllb_model, tokenizer=nllb_tokenizer, \n",
    "                                       src_lang='olo_Latn', tgt_lang='rus_Cyrl')\n",
    "        mbart_trans = batched_translate(texts, 16, model=mbart_model, tokenizer=mbart_tokenizer, \n",
    "                                src_lang='fi_FI', tgt_lang='ru_RU')\n",
    "        pairs = list()\n",
    "        for text in texts:\n",
    "            pairs.append(get_dict_translate(text))\n",
    "    return nllb_trans, mbart_trans, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6c76df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nevvostoliitos kehitettih omaluaduine, pehmei taba kuvata liikehty, kudamas tuli muan animacien tunnusmerki.\\n',\n",
       " 'Animaciitehniekkoi.\\n',\n",
       " 'Animaciitehniekkoi on monenluadustu da niilöi voi sežo yhtistellä.\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_sentence = lines[0].strip()\n",
    "sentences = lines[0:3]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4b8e3b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d0f210a8cf4ec2a4673758f3831aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984dc33149c544cca47f2beac749fab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e8c7effd6b417eb394a6b7c87ef4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630773407eb44063809bbddc0571b6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nllb, mbart, pairs = get_translates(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3820e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines = [line.strip() for line in lines if len(line.split()) != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 110\n",
    "nllb_trans, mbart_trans, pairs_trans = list(), list(), list()\n",
    "batch_size = 16\n",
    "for line_id in tqdm(range(0, N, batch_size)):\n",
    "    sentences = new_lines[line_id:line_id+batch_size]\n",
    "    nllb, mbart, pairs = get_translates(sentences)\n",
    "    nllb_trans.extend(nllb)\n",
    "    mbart_trans.extend(mbart)\n",
    "    pairs_trans.extend(pairs)\n",
    "    if len(nllb_trans) >= 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1cd12e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLLB</th>\n",
       "      <th>MBART</th>\n",
       "      <th>Pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>В бракосочетании разработали своеобразный, мяг...</td>\n",
       "      <td>В совете вырабатывали своеобразный, мягкий обр...</td>\n",
       "      <td>[(Nevvostoliitos, кон — кон (учреждение)), (ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Красные приспособления.</td>\n",
       "      <td>runrunrunсерсерваторы.</td>\n",
       "      <td>[(Animaciitehniekkoi., теплотехник)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Животные инженеры - многообразные, и их также ...</td>\n",
       "      <td>runника есть многоразовая, и её можно тоже сое...</td>\n",
       "      <td>[(Animaciitehniekkoi, теплотехник), (on, есть ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                NLLB  \\\n",
       "0  В бракосочетании разработали своеобразный, мяг...   \n",
       "1                            Красные приспособления.   \n",
       "2  Животные инженеры - многообразные, и их также ...   \n",
       "\n",
       "                                               MBART  \\\n",
       "0  В совете вырабатывали своеобразный, мягкий обр...   \n",
       "1                             runrunrunсерсерваторы.   \n",
       "2  runника есть многоразовая, и её можно тоже сое...   \n",
       "\n",
       "                                               Pairs  \n",
       "0  [(Nevvostoliitos, кон — кон (учреждение)), (ke...  \n",
       "1               [(Animaciitehniekkoi., теплотехник)]  \n",
       "2  [(Animaciitehniekkoi, теплотехник), (on, есть ...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame([nllb, mbart, pairs]).T\n",
    "res.columns = ['NLLB', 'MBART', 'Pairs']\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karelian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10911950,"sourceType":"datasetVersion","datasetId":6732768},{"sourceId":11788176,"sourceType":"datasetVersion","datasetId":6742498},{"sourceId":11852441,"sourceType":"datasetVersion","datasetId":7107905},{"sourceId":11857886,"sourceType":"datasetVersion","datasetId":7383987},{"sourceId":392169,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":244953,"modelId":253734}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install sacremoses sacrebleu --quiet","metadata":{"execution":{"iopub.status.busy":"2025-05-22T21:32:24.344592Z","iopub.execute_input":"2025-05-22T21:32:24.345346Z","iopub.status.idle":"2025-05-22T21:32:28.995702Z","shell.execute_reply.started":"2025-05-22T21:32:24.345318Z","shell.execute_reply":"2025-05-22T21:32:28.994880Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import gc\nimport torch\nimport re\nimport random\nimport sys\nfrom collections import Counter\nimport unicodedata\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm, trange\nfrom sklearn.model_selection import train_test_split\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, NllbTokenizer\nfrom transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nfrom transformers import get_constant_schedule_with_warmup\nfrom transformers.optimization import Adafactor\n\nimport sentencepiece as spm\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n\nfrom torch.optim.lr_scheduler import SequentialLR, ExponentialLR, ConstantLR, LinearLR\nfrom sacremoses import MosesPunctNormalizer\n\nimport json\nimport os\nimport shutil\nfrom typing import List, Tuple\n\nfrom transformers.models.nllb.tokenization_nllb import FAIRSEQ_LANGUAGE_CODES\n\ndef cleanup():\n    \"\"\"Try to free GPU memory\"\"\"\n    gc.collect()\n    torch.cuda.empty_cache()\ncleanup()","metadata":{"execution":{"iopub.status.busy":"2025-05-22T21:32:28.996856Z","iopub.execute_input":"2025-05-22T21:32:28.997074Z","iopub.status.idle":"2025-05-22T21:32:29.570405Z","shell.execute_reply.started":"2025-05-22T21:32:28.997055Z","shell.execute_reply":"2025-05-22T21:32:29.569877Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# res = pd.read_csv('/kaggle/input/wiki-to-label/res.csv')\n# tartu_trans = pd.read_csv('/kaggle/input/wiki-to-label/tartu_trans.csv',index_col=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T23:19:59.095210Z","iopub.execute_input":"2025-05-10T23:19:59.095767Z","iopub.status.idle":"2025-05-10T23:19:59.313628Z","shell.execute_reply.started":"2025-05-10T23:19:59.095745Z","shell.execute_reply":"2025-05-10T23:19:59.313089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trans_10k_all = pd.concat([res, tartu_trans],axis=1)\n# trans_10k_all.to_csv('trans_10k_all.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T23:20:42.766097Z","iopub.execute_input":"2025-05-10T23:20:42.766715Z","iopub.status.idle":"2025-05-10T23:20:42.994115Z","shell.execute_reply.started":"2025-05-10T23:20:42.766691Z","shell.execute_reply":"2025-05-10T23:20:42.993527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KAGGLE_INPUT = '/kaggle/input/'\nMODEL_PATH = 'nllb-rus-kar'\nKRL = KAGGLE_INPUT + 'karelian-data/'\nDF_PATH = KRL + 'parallel_df_merged4.csv'\n# MODEL_LOAD_PATH = KAGGLE_INPUT + '/m/taciturno/nllb-rus-kar/pytorch/ft-messy/1'\nMODEL_SAVE_PATH = MODEL_PATH","metadata":{"execution":{"iopub.status.busy":"2025-05-22T21:32:32.848327Z","iopub.execute_input":"2025-05-22T21:32:32.848894Z","iopub.status.idle":"2025-05-22T21:32:32.852589Z","shell.execute_reply.started":"2025-05-22T21:32:32.848866Z","shell.execute_reply":"2025-05-22T21:32:32.851945Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"df_corpus_labeled = pd.read_csv(DF_PATH)\ndf_train = df_corpus_labeled[df_corpus_labeled.split=='train'].copy() # 22692 items\ndf_dev = df_corpus_labeled[df_corpus_labeled.split=='dev'].copy()     # 500 items\ndf_test = df_corpus_labeled[df_corpus_labeled.split=='test'].copy()  \n\ndf_dev = df_dev.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2025-05-22T21:32:32.996449Z","iopub.execute_input":"2025-05-22T21:32:32.997074Z","iopub.status.idle":"2025-05-22T21:32:33.328293Z","shell.execute_reply.started":"2025-05-22T21:32:32.997046Z","shell.execute_reply":"2025-05-22T21:32:33.327725Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# df_train = trans_10k_all[['orig', 'rus']].copy()\n# df_train.columns = ['kar', 'rus']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T23:23:47.336404Z","iopub.execute_input":"2025-05-10T23:23:47.337117Z","iopub.status.idle":"2025-05-10T23:23:47.343351Z","shell.execute_reply.started":"2025-05-10T23:23:47.337086Z","shell.execute_reply":"2025-05-10T23:23:47.342746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preproc","metadata":{}},{"cell_type":"code","source":"mpn = MosesPunctNormalizer(lang=\"en\")\nmpn.substitutions = [\n    (re.compile(r), sub) for r, sub in mpn.substitutions\n]\n\ndef get_non_printing_char_replacer(replace_by: str = \" \"):\n    non_printable_map = {\n        ord(c): replace_by\n        for c in (chr(i) for i in range(sys.maxunicode + 1))\n        # same as \\p{C} in perl\n        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n    }\n\n    def replace_non_printing_char(line) -> str:\n        return line.translate(non_printable_map)\n\n    return replace_non_printing_char\n\nreplace_nonprint = get_non_printing_char_replacer(\" \")\n\ndef preproc(text):\n    clean = mpn.normalize(text)\n    clean = replace_nonprint(clean)\n    # replace ùìïùîØùîûùî´ùî†ùî¢ùî∞ùî†ùîû by Francesca\n    clean = unicodedata.normalize(\"NFKC\", clean)\n    return clean","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:31.149645Z","iopub.execute_input":"2025-05-10T23:26:31.150158Z","iopub.status.idle":"2025-05-10T23:26:31.472782Z","shell.execute_reply.started":"2025-05-10T23:26:31.150135Z","shell.execute_reply":"2025-05-10T23:26:31.472218Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Tokenizer","metadata":{}},{"cell_type":"code","source":"additional_special_tokens=sorted(FAIRSEQ_LANGUAGE_CODES + ['olo_Latn'])","metadata":{"execution":{"iopub.execute_input":"2025-05-05T19:19:27.774262Z","iopub.status.busy":"2025-05-05T19:19:27.774032Z","iopub.status.idle":"2025-05-05T19:19:27.777927Z","shell.execute_reply":"2025-05-05T19:19:27.777147Z","shell.execute_reply.started":"2025-05-05T19:19:27.774244Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = 'facebook/nllb-200-distilled-600M' # 'facebook/nllb-200-distilled-1.3B' #\nNEW_SPM_NAME = '/kaggle/input/karelian-data/spm_nllb_karelian.model'\n# loading the tokenizers\ntokenizer_old = NllbTokenizer.from_pretrained(model_name)\ntokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file=NEW_SPM_NAME, additional_special_tokens=additional_special_tokens)\nprint(len(tokenizer_old), len(tokenizer)) # 256204, 268559\nadded_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))\nprint(len(added_vocab))  # 13011\n\n# loading and resizing the model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# re-initializing the new embeddings\nfor t in tqdm(added_vocab):\n    tt = tokenizer_old(t, add_special_tokens=False).input_ids\n    if len(tt) == 0:\n        tt = [tokenizer_old.unk_token_id]\n    idx = tokenizer.convert_tokens_to_ids(t)\n    model.model.shared.weight.data[idx] = model.model.shared.weight.data[tt].mean(0)","metadata":{"execution":{"iopub.execute_input":"2025-05-05T19:19:33.232917Z","iopub.status.busy":"2025-05-05T19:19:33.232635Z","iopub.status.idle":"2025-05-05T19:20:06.945542Z","shell.execute_reply":"2025-05-05T19:20:06.944543Z","shell.execute_reply.started":"2025-05-05T19:19:33.232895Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"added_token_id = tokenizer.convert_tokens_to_ids('olo_Latn')\nsimilar_lang_id = tokenizer.convert_tokens_to_ids('fin_Latn')\nprint(model.model.shared.weight.data[added_token_id])\nmodel.model.shared.weight.data[added_token_id] = model.model.shared.weight.data[similar_lang_id]\nprint(model.model.shared.weight.data[added_token_id])","metadata":{"execution":{"iopub.execute_input":"2025-05-05T19:20:06.947219Z","iopub.status.busy":"2025-05-05T19:20:06.946929Z","iopub.status.idle":"2025-05-05T19:20:06.964561Z","shell.execute_reply":"2025-05-05T19:20:06.963680Z","shell.execute_reply.started":"2025-05-05T19:20:06.947196Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"# cleanup()\nNLLB_NEW_PATH = '/kaggle/input/nllb-rus-kar/pytorch/dict/5'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(NLLB_NEW_PATH).cuda()\ntokenizer = NllbTokenizer.from_pretrained(NLLB_NEW_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:34:14.664517Z","iopub.execute_input":"2025-05-22T21:34:14.665222Z","iopub.status.idle":"2025-05-22T21:34:33.751062Z","shell.execute_reply.started":"2025-05-22T21:34:14.665197Z","shell.execute_reply":"2025-05-22T21:34:33.750413Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model.loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T21:34:39.971575Z","iopub.execute_input":"2025-05-22T21:34:39.971909Z","iopub.status.idle":"2025-05-22T21:34:39.998522Z","shell.execute_reply.started":"2025-05-22T21:34:39.971880Z","shell.execute_reply":"2025-05-22T21:34:39.997347Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4226091950.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n","\u001b[0;31mAttributeError\u001b[0m: 'M2M100ForConditionalGeneration' object has no attribute 'loss'"],"ename":"AttributeError","evalue":"'M2M100ForConditionalGeneration' object has no attribute 'loss'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"LR = 1e-4\n\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=LR,\n    clip_threshold=1.0,\n    weight_decay=1e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)\nscheduler1 = ConstantLR(optimizer, factor=1, total_iters=1000)\nscheduler2 = ConstantLR(optimizer, factor=0.9, total_iters=8000)\nscheduler3 = ConstantLR(optimizer, factor=0.8, total_iters=1000)\nscheduler4 = ConstantLR(optimizer, factor=0.7, total_iters=11000)\nscheduler = SequentialLR(optimizer, \n                         schedulers=[scheduler1, scheduler2, scheduler3, scheduler4], \n                         milestones=[1000,9000,10000])\n# scheduler = SequentialLR(optimizer, \n#                          schedulers=[scheduler1, scheduler2, scheduler3, scheduler4], \n#                          milestones=[1000,9000,10000])\n# scheduler = scheduler1","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:20.713761Z","iopub.execute_input":"2025-05-10T23:26:20.714026Z","iopub.status.idle":"2025-05-10T23:26:20.728098Z","shell.execute_reply.started":"2025-05-10T23:26:20.714003Z","shell.execute_reply":"2025-05-10T23:26:20.727509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LANGS = [('rus', 'rus_Cyrl'), ('kar', 'olo_Latn')]\n# TODO –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å –≤ dataset\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, lang1), (l2, lang2) = random.sample(LANGS, 2) # –∑–¥–µ—Å—å random —á—Ç–æ–±—ã –ø–µ—Ä–µ–≤–æ–¥–∏–ª–∞ –º–æ–¥–µ–ª—å —Ç—É–¥–∞-—Å—é–¥–∞\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(preproc(item[l1]))\n        yy.append(preproc(item[l2]))\n    return xx, yy, lang1, lang2\n\nprint(get_batch_pairs(1))","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:39.660289Z","iopub.execute_input":"2025-05-10T23:26:39.660815Z","iopub.status.idle":"2025-05-10T23:26:39.667506Z","shell.execute_reply.started":"2025-05-10T23:26:39.660791Z","shell.execute_reply":"2025-05-10T23:26:39.666288Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BATCH_SIZE = 8  # 32 already doesn't fit well to 15GB of GPU memory\nMAX_LENGTH = 128 \nTRAINING_STEPS = 21000\n\nN_STEPS_TO_ESTIMATE = 100\nlosses = list()\n\npreproc(df_dev.iloc[0]['kar'])","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:41.453348Z","iopub.execute_input":"2025-05-10T23:26:41.453881Z","iopub.status.idle":"2025-05-10T23:26:41.459153Z","shell.execute_reply.started":"2025-05-10T23:26:41.453861Z","shell.execute_reply":"2025-05-10T23:26:41.458469Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_batched_validation(batch_size):\n    (l1, lang1), (l2, lang2) = random.sample(LANGS, 2)\n    for i in range(0, len(df_dev), batch_size):\n        xx, yy = list(), list()\n        sl = df_dev.iloc[i:i+batch_size]\n        for _, row in sl.iterrows():\n            xx.append(preproc(row[l1]))\n            yy.append(preproc(row[l2]))\n        yield xx, yy, lang1, lang2","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:47.618729Z","iopub.execute_input":"2025-05-10T23:26:47.619371Z","iopub.status.idle":"2025-05-10T23:26:47.624012Z","shell.execute_reply.started":"2025-05-10T23:26:47.619344Z","shell.execute_reply":"2025-05-10T23:26:47.623263Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validation(model, data=df_dev, batch_size=16):\n    model.eval()\n    av_loss = list()\n    with torch.no_grad():\n        for xx, yy, lang1, lang2 in get_batched_validation(batch_size):#range(len(data) // batch_size):\n            # xx, yy, lang1, lang2 =  #get_batch_pairs(batch_size, data)\n            tokenizer.src_lang = lang1\n            x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to('cuda')\n            tokenizer.src_lang = lang2\n            y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to('cuda')\n            y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100 # TODO: –Ω–∞–¥–æ –ª–∏?\n            loss = model(**x, labels=y.input_ids).loss\n            av_loss.append(loss.item())\n    model.train()\n    return np.mean(av_loss)","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:48.574873Z","iopub.execute_input":"2025-05-10T23:26:48.575847Z","iopub.status.idle":"2025-05-10T23:26:48.581194Z","shell.execute_reply.started":"2025-05-10T23:26:48.575809Z","shell.execute_reply":"2025-05-10T23:26:48.580476Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:50.603159Z","iopub.execute_input":"2025-05-10T23:26:50.603839Z","iopub.status.idle":"2025-05-10T23:26:50.607239Z","shell.execute_reply.started":"2025-05-10T23:26:50.603815Z","shell.execute_reply":"2025-05-10T23:26:50.606500Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_lr(optimizer)","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:50.772639Z","iopub.execute_input":"2025-05-10T23:26:50.772903Z","iopub.status.idle":"2025-05-10T23:26:50.778065Z","shell.execute_reply.started":"2025-05-10T23:26:50.772884Z","shell.execute_reply":"2025-05-10T23:26:50.777249Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\nx, y, loss = None, None, None\nBEST_VAL_LOSS = 1e9\nBEST_TRAIN_LOSS = 1e9\ncleanup()\n\n\ntq = trange(len(losses), TRAINING_STEPS)\nfor step in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(BATCH_SIZE)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=MAX_LENGTH).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e: \n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n    if step % 1000 == 0:\n        print(get_lr(optimizer))\n    if step < 10000 and step % N_STEPS_TO_ESTIMATE == 0:\n        # average for N steps\n        train_loss = np.mean(losses[-N_STEPS_TO_ESTIMATE:])\n        val_loss = validation(model)\n        print(f'Train loss is {train_loss} and valid loss is {val_loss} at step {step}')\n        if train_loss < 1.75 and train_loss < BEST_TRAIN_LOSS and val_loss < BEST_VAL_LOSS:\n            BEST_TRAIN_LOSS = train_loss\n            BEST_VAL_LOSS = val_loss\n            model.save_pretrained(MODEL_SAVE_PATH)\n            tokenizer.save_pretrained(MODEL_SAVE_PATH)\n            print(f'model_saved')\n    elif step >= 10000 and step % 300 == 0:\n        train_loss = np.mean(losses[-300:])\n        val_loss = validation(model)\n        print(f'Train loss is {train_loss} and valid loss is {val_loss} at step {step}')\n        if train_loss < 1.75 and train_loss < BEST_TRAIN_LOSS and val_loss < BEST_VAL_LOSS:\n            BEST_TRAIN_LOSS = train_loss\n            BEST_VAL_LOSS = val_loss\n            model.save_pretrained(MODEL_SAVE_PATH)\n            tokenizer.save_pretrained(MODEL_SAVE_PATH)\n            print(f'model_saved')","metadata":{"execution":{"iopub.status.busy":"2025-05-10T23:26:52.300718Z","iopub.execute_input":"2025-05-10T23:26:52.301015Z","execution_failed":"2025-05-10T23:31:15.930Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}